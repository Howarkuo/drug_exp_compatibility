import pandas as pd
import numpy as np
import os
import joblib
import umap
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from rdkit import Chem
from rdkit import RDLogger
from mordred import Calculator, descriptors
from tqdm import tqdm

RDLogger.DisableLog('rdApp.*')

# ==========================================
# 1. æª”æ¡ˆè·¯å¾‘è¨­å®š
# ==========================================
TRAIN_FILE = '2024_Drug_compatibility_dataset.xlsx'
TEST_FILE = 'Mordered/0210_mordred_rf_prediction_results.csv' # ä½¿ç”¨æ‚¨ä¸Šä¸€éƒ¨è·‘å®Œã€æœ‰ SMILES çš„çµæœæª”
FEATURE_FILE = 'Mordered/0210_mordred_features_list.pkl'      # å°é½Šç‰¹å¾µç”¨

# ==========================================
# 2. ç‰¹å¾µè¨ˆç®—å‡½æ•¸ (èˆ‡è¨“ç·´æ™‚ç›¸åŒ)
# ==========================================
def generate_mordred_features(smiles_list, prefix):
    print(f"âš™ï¸ Generating {prefix} features...")
    mols = [Chem.MolFromSmiles(str(s)) if pd.notna(s) else None for s in smiles_list]
    valid_mols = [m for m in mols if m is not None]
    
    calc = Calculator(descriptors, ignore_3D=True)
    df_features = calc.pandas(valid_mols, nproc=1, quiet=True).apply(pd.to_numeric, errors='coerce').fillna(0)
    
    final_df = pd.DataFrame(0, index=range(len(smiles_list)), columns=df_features.columns)
    valid_indices = [i for i, m in enumerate(mols) if m is not None]
    final_df.iloc[valid_indices] = df_features.values
    return final_df.add_prefix(prefix)

# ==========================================
# 3. ä¸»ç¨‹å¼
# ==========================================
if __name__ == "__main__":
    
    print("ğŸ“‚ Loading Required Features List...")
    if not os.path.exists(FEATURE_FILE):
        print("âŒ Feature list not found!"); exit()
    required_features = joblib.load(FEATURE_FILE)

    # --- A. è™•ç† Test Set (162ç­†) ---
    print(f"\nğŸ“‚ Loading Test Set: {TEST_FILE}")
    df_test = pd.read_csv(TEST_FILE)
    df_test_api = generate_mordred_features(df_test['API_SMILES'].tolist(), "API_")
    df_test_exp = generate_mordred_features(df_test['EXP_SMILES'].tolist(), "EXP_")
    X_test_raw = pd.concat([df_test_api, df_test_exp], axis=1)
    X_test = X_test_raw.reindex(columns=required_features, fill_value=0)
    
    # --- B. è™•ç† Train Set (3544ç­†) ---
    print(f"\nğŸ“‚ Loading Train Set: {TRAIN_FILE}")
    df_train = pd.read_excel(TRAIN_FILE)
    # é€™è£¡æœƒè·‘å€‹ 10 åˆ†é˜ï¼Œè«‹è€å¿ƒç­‰å€™
    df_train_api = generate_mordred_features(df_train['API_Smiles'].tolist(), "API_")
    df_train_exp = generate_mordred_features(df_train['Excipient_Smiles'].tolist(), "EXP_")
    X_train_raw = pd.concat([df_train_api, df_train_exp], axis=1)
    X_train = X_train_raw.reindex(columns=required_features, fill_value=0)

    # # --- C. åˆä½µä¸¦æ¨™è¨˜ä¾†æº ---
    # print("\nğŸ”— Combining and Scaling Data...")
    # X_train['Dataset'] = 'Training Data (3544 items)'
    # X_test['Dataset'] = 'Validation Data (162 items)'
    
    # X_combined = pd.concat([X_train, X_test], axis=0).reset_index(drop=True)
    # labels = X_combined['Dataset'].values
    
    # # ç§»é™¤ Dataset æ¨™ç±¤ä¸¦é€²è¡Œæ¨™æº–åŒ– (UMAP å¿…é ˆå…ˆæ¨™æº–åŒ–)
    # X_features = X_combined.drop(columns=['Dataset'])
    # scaler = StandardScaler()
    # X_scaled = scaler.fit_transform(X_features)

    # # --- D. åŸ·è¡Œ UMAP é™ç¶­ ---
    # print("ğŸ—ºï¸ Running UMAP Dimension Reduction (Transforming ~2700D to 2D)...")
    # reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)
    # embedding = reducer.fit_transform(X_scaled)

    # --- C. åˆ†é–‹æ¨™æº–åŒ– (åš´è¬¹åšæ³•: Scaler åª fit Train) ---
    print("\nğŸ”— Scaling Data (Fit on Train, Transform on Test)...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test) # åªåš transform

    # --- D. åŸ·è¡Œ UMAP é™ç¶­ (åš´è¬¹åšæ³•: UMAP åª fit Train) ---
    print("ğŸ—ºï¸ Running UMAP Dimension Reduction...")
    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)
    
    # 1. å…ˆæŠŠè¨“ç·´é›†é™ç¶­ (Fit + Transform)
    embedding_train = reducer.fit_transform(X_train_scaled)
    
    # 2. æŠŠæ¸¬è©¦é›†æŠ•å½±åˆ°å‰›å»ºå¥½çš„ç©ºé–“ä¸­ (Transform Only)
    embedding_test = reducer.transform(X_test_scaled)
    
    # 3. æŠŠåº§æ¨™åˆä½µèµ·ä¾†æº–å‚™ç•«åœ–
    embedding = np.vstack((embedding_train, embedding_test))
    
    # å»ºç«‹æ¨™ç±¤
    labels = ['Training Data (3544 items)'] * len(X_train) + ['Validation Data (162 items)'] * len(X_test)

    # --- E. ç¹ªè£½ç²¾ç¾æ•£ä½ˆåœ– ---
    print("ğŸ¨ Plotting visualization...")
    plt.figure(figsize=(10, 8))
    
    # ä½¿ç”¨ seaborn ç¹ªè£½ï¼Œèª¿æ•´é»çš„å¤§å°èˆ‡é€æ˜åº¦å‡¸é¡¯ 162 ç­†æ¸¬è©¦é›†
    sns.scatterplot(
        x=embedding[:, 0], y=embedding[:, 1],
        hue=labels,
        palette={'Training Data (3544 items)': '#B0BEC5', 'Validation Data (162 items)': '#E53935'},
        alpha=0.7,
        s=[20 if l == 'Training Data (3544 items)' else 80 for l in labels],
        edgecolor=None
    )

    plt.title('UMAP Chemical Space: Training vs. Validation (162 items)', fontsize=14, fontweight='bold')
    plt.xlabel('UMAP Dimension 1', fontsize=12)
    plt.ylabel('UMAP Dimension 2', fontsize=12)
    plt.legend(title='Dataset Origin', fontsize=10, title_fontsize=12)
    plt.tight_layout()

    # å­˜æª”
    output_img = 'UMAP_Train_vs_Test_Distribution.png'
    plt.savefig(output_img, dpi=300)
    print(f"âœ… Success! Map saved to: {output_img}")